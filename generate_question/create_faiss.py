import os
import json
import traceback
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# error log file
ERROR_LOG_FILE = "error_log.txt"
BATCH_SIZE = 100
FAISS_PATH = "/mnt/efs_faiss_index/"

# OpenAI API Key ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")
embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)


def load_or_create_faiss_index(save_path):
    if os.path.exists(save_path) and os.path.isdir(save_path):
        print(f"ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤: {save_path}")
        return FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)
    else:
        print(f"ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {save_path}")
        return None  # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ None ë°˜í™˜


# ë‰´ìŠ¤ ë°ì´í„° FAISS ì €ì¥/ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def create_or_update_news_faiss(news_articles, save_path=FAISS_PATH + "news_faiss"):
    # ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œ
    news_faiss = load_or_create_faiss_index(save_path)

    existing_entries = set()

    if news_faiss:
        # ê¸°ì¡´ ë‰´ìŠ¤ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        docs = news_faiss.similarity_search("", k=1000)  # FAISSì—ì„œ ìµœëŒ€ 1000ê°œ ê²€ìƒ‰
        existing_entries = {
            (doc.page_content, doc.metadata["press"], doc.metadata["date"], doc.metadata["keywords"],
             doc.metadata["section"])
            for doc in docs
        }
        print(f"ê¸°ì¡´ ë‰´ìŠ¤ ê°œìˆ˜: {len(existing_entries)}")

    # ì¤‘ë³µì„ ì œê±°í•œ ìƒˆë¡œìš´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    new_news_documents = [
        Document(
            page_content=article['content'],  # ì œëª© + ë³¸ë¬¸ ê²°í•©
            metadata={
                "press": article["press"],
                "date": article["date"],
                "keywords": article["keywords"],
                "section": article["section"]
            }
        )
        for article in news_articles
        if (article['content'], article["press"], article["date"], article["keywords"],
            article["section"]) not in existing_entries
    ]

    if new_news_documents:
        if news_faiss:
            news_faiss.add_documents(new_news_documents)  # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
        else:
            news_faiss = FAISS.from_documents(new_news_documents, embeddings)  # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±

        news_faiss.save_local(save_path)
        print(f"{len(new_news_documents)}ê°œì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ëª¨ë“  ë‰´ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì¶”ê°€í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")


# í‚¤ì›Œë“œ FAISS ì €ì¥/ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def create_or_update_keyword_faiss(news_articles, save_path=FAISS_PATH + "keyword_faiss"):
    # ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ (ì—†ìœ¼ë©´ None)
    keyword_faiss = load_or_create_faiss_index(save_path)

    existing_entries = set()

    if keyword_faiss:
        # ê¸°ì¡´ í‚¤ì›Œë“œ + ë©”íƒ€ë°ì´í„° ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        docs = keyword_faiss.similarity_search("", k=1000)  # ì „ì²´ ê²€ìƒ‰
        existing_entries = {(doc.page_content, doc.metadata["press"], doc.metadata["date"], doc.metadata["section"]) for
                            doc in docs}
        print(f"ğŸ“Œ ê¸°ì¡´ í‚¤ì›Œë“œ ê°œìˆ˜: {len(existing_entries)}")

    # ì¤‘ë³µ ì œê±°í•˜ì—¬ ìƒˆë¡œìš´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    new_keywords = [
        Document(
            page_content=article["keywords"],
            metadata={"press": article["press"], "date": article["date"], "section": article["section"]}
        )
        for article in news_articles
        # í‚¤ì›Œë“œ + ë©”íƒ€ë°ì´í„° ì¤‘ë³µ ë°©ì§€
        if (article["keywords"], article["press"], article["date"], article["section"]) not in existing_entries
    ]

    if new_keywords:
        if keyword_faiss:
            keyword_faiss.add_documents(new_keywords)  # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
        else:
            keyword_faiss = FAISS.from_documents(new_keywords, embeddings)  # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±

        keyword_faiss.save_local(save_path)
        print(f"{len(new_keywords)}ê°œì˜ ìƒˆë¡œìš´ í‚¤ì›Œë“œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("ëª¨ë“  í‚¤ì›Œë“œ(ë©”íƒ€ë°ì´í„° í¬í•¨)ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì¶”ê°€í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")


# ì˜¤ë¥˜ ë¡œê·¸ ê¸°ë¡ í•¨ìˆ˜
def log_error(batch_index, article, error):
    with open(ERROR_LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"\n[Batch {batch_index}] ì˜¤ë¥˜ ë°œìƒ!\n")
        f.write(f"ì–¸ë¡ ì‚¬: {article.get('press', 'N/A')}\n")
        f.write(f"ë‚ ì§œ: {article.get('date', 'N/A')}\n")
        f.write(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {error}\n")
        f.write(f"{traceback.format_exc()}\n")
    print(f"ì˜¤ë¥˜ ë°œìƒ! Batch {batch_index} - ê¸°ì‚¬: {article.get('title', 'N/A')} (ë¡œê·¸ ì €ì¥ë¨)")


# JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ Batch ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def process_news_json(json_path):
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            news_articles = json.load(f)

        total_articles = len(news_articles)
        print(f"ğŸ“° ì´ {total_articles}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        for i in range(0, total_articles, BATCH_SIZE):
            batch = news_articles[i:i + BATCH_SIZE]
            batch_index = i // BATCH_SIZE + 1
            print(f"\n[Batch {batch_index}] {i + 1}ë²ˆ ~ {i + len(batch)}ë²ˆ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")

            try:
                create_or_update_news_faiss(batch)
                create_or_update_keyword_faiss(batch)
                print(f"[Batch {batch_index}] ì €ì¥ ì™„ë£Œ!")
            except Exception as e:
                for article in batch:
                    log_error(batch_index, article, str(e))

    except Exception as e:
        log_error("ì „ì²´", {}, str(e))
        print(f"ì „ì²´ JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ! {str(e)}")


# ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    json_file_path = "news_data.json"  # JSON íŒŒì¼ ê²½ë¡œ
    process_news_json(json_file_path)
