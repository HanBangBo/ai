import os
from collections import Counter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# OpenAI API Key ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")
save_path = "//mnt/efs_faiss_index//keyword_faiss"

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ
embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
vector_db = FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)


def create_keywords(keyword, source_type, source_value):
    """
    âœ… ì‚¬ìš©ì ë§ì¶¤í˜• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

    Parameters:
        - params (dict): ë°±ì—”ë“œì—ì„œ ë„˜ì–´ì˜¨ ë°ì´í„°
          - user (str): ì‚¬ìš©ì ID
          - source_type (str): 'ì–¸ë¡ ì‚¬' ë˜ëŠ” 'ì¹´í…Œê³ ë¦¬'
          - source_value (list): ì„ íƒëœ ì–¸ë¡ ì‚¬ or ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸
          - keyword (dict): {í‚¤ì›Œë“œ: ì˜¤ë‹µë¥ } í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬

    Returns:
        - list: ìµœì¢… 10ê°œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """

    additional_keywords = []

    # âœ… FAISS ë²¡í„°DBì—ì„œ í•„í„°ë§í•˜ì—¬ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    if source_type == "ì–¸ë¡ ì‚¬":
        results = vector_db.similarity_search("", k=100, filter={"press": source_value})
        filtered_results = [
            doc.page_content for doc in results if doc.metadata.get("press") == source_value
        ]
        print(f"ğŸ“Œ ì–¸ë¡ ì‚¬ '{source_value}'ì—ì„œ ê°€ì ¸ì˜¨ í‚¤ì›Œë“œ ê°œìˆ˜: {len(filtered_results)}")  # âœ… ë””ë²„ê¹…
        additional_keywords.extend(filtered_results)

    elif source_type == "ì¹´í…Œê³ ë¦¬":
        results = vector_db.similarity_search("", k=100, filter={"section": source_value})
        filtered_results = [
            doc.page_content for doc in results if doc.metadata.get("section") == source_value
        ]
        print(f"ğŸ“Œ ì¹´í…Œê³ ë¦¬ '{source_value}'ì—ì„œ ê°€ì ¸ì˜¨ í‚¤ì›Œë“œ ê°œìˆ˜: {len(filtered_results)}")  # âœ… ë””ë²„ê¹…
        additional_keywords.extend(filtered_results)

    # âœ… í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ ê³„ì‚° í›„ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í‚¤ì›Œë“œ ì¶”ì¶œ
    keyword_counts = Counter(additional_keywords)
    sorted_keywords = [kw for kw, _ in keyword_counts.most_common()]


    final_keyword_list = []
    for key in keyword:
        final_keyword_list.append(key)

    # âœ… 10ê°œ ì´ìƒ ì±„ìš°ê¸°
    for kw in sorted_keywords:
        if len(final_keyword_list) < 10 and kw not in keyword:
            final_keyword_list.append(kw)

    if len(final_keyword_list) < 10:
        extra_results = vector_db.similarity_search("", k=200)  # ë” ë§ì€ ë°ì´í„° ê²€ìƒ‰
        extra_keywords = [doc.page_content for doc in extra_results]
        extra_keyword_counts = Counter(extra_keywords)
        sorted_extra_keywords = [kw for kw, _ in extra_keyword_counts.most_common()]

        for kw in sorted_extra_keywords:
            if len(final_keyword_list) < 10 and kw not in final_keyword_list:
                final_keyword_list.append(kw)

    return final_keyword_list[:10]
