# -*- coding: utf-8 -*-
"""ë‰´ìŠ¤í¬ë¡¤ë§_í‚¤ì›Œë“œì¶”ì¶œ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hAFSFOpSjnO38HAocK0J_U99at91VmU0
"""

! pip install webdriver-manager
! apt-get update
! apt-get install -y wget unzip
! pip install requests beautifulsoup4 pandas lxml
! pip install ace_tools

! pip install langchain_openai

import os

os.environ['OPENAI_API_KEY'] = "OPENAI_API_KEY"

import requests
from bs4 import BeautifulSoup
import datetime

# âœ… ì–¸ë¡ ì‚¬ ì½”ë“œ ì„¤ì • (ì˜ˆ: í—¤ëŸ´ë“œê²½ì œ -> 016)
MEDIA_CODE = "016"

# âœ… í¬ë¡¤ë§í•  ê¸°ê°„ ì„¤ì • (ìµœê·¼ 3ê°œì›”)
today = datetime.date.today()
date_range = [(today - datetime.timedelta(days=i)).strftime('%Y%m%d') for i in range(180)]

# âœ… ê¸°ì‚¬ URL ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
def get_news_urls(news_date):
    NAVER_RANKING_URL = f"https://media.naver.com/press/{MEDIA_CODE}/ranking?type=section&date={news_date}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36"
    }

    response = requests.get(NAVER_RANKING_URL, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    news_data = []
    section_map = {
        3: "ì •ì¹˜",
        4: "ê²½ì œ",
        5: "ì‚¬íšŒ",
        6: "êµ­ì œ",
        7: "ë¬¸í™”",
        8: "ê³¼í•™"
    }

    found_news = False

    for section in range(3, 9):
      for rank in range(1, 4):
        selector = f"#ct > div.press_ranking_home > div:nth-child({section}) > ul > li:nth-child({rank}) > a"
        link = soup.select_one(selector)

        if link:
          url = link.get("href")
          if url and "article" in url:
            full_url = "https://n.news.naver.com" + url if url.startswith("/article") else url

            news_data.append({
                "press": "í—¤ëŸ´ë“œê²½ì œ",
                "date": news_date,
                "section": section_map.get(section, "ê¸°íƒ€"),
                "url": full_url
              })
            found_news = True

    if not found_news:
      print(f"{news_date}ì— ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‚ ì§œë¡œ ì´ë™í•©ë‹ˆë‹¤.")

    return news_data

# âœ… ìµœê·¼ 30ì¼ê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰
all_news_urls = []
for news_date in date_range:
    news_urls = get_news_urls(news_date)
    all_news_urls.extend(news_urls)

# âœ… DataFrame ë³€í™˜ ë° ì¶œë ¥
news_url_df = pd.DataFrame(all_news_urls)

# âœ… ë°ì´í„° í™•ì¸
from IPython.display import display
display(news_url_df)

import re
import requests
import pandas as pd
from bs4 import BeautifulSoup

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                        'Chrome/102.0.0.0 Safari/537.36'}


def extract_korean_news(url):
    try:
        get_url = requests.get(url=url, headers=headers)
    except requests.exceptions.RequestException as e:
        print("ìš”ì²­ ì‹¤íŒ¨")
        return None

    bs = BeautifulSoup(get_url.text, 'lxml')
    content = bs.find('article', id='dic_area')

    if content is None:
      print("ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
      return None

    unexpectes = [content.find('strong')] + content.find_all('span', {'class': 'end_photo_org'})
    for exception in unexpectes:
        if exception is not None:
            exception.extract()

    content = content.text
    for i in range(2):
        content = re.sub('  ', ' ', content)
    content = re.sub('\n|\t|\xa0', ' ', content)

    return {'content': content, 'url': url}

news_data = []
for url in news_url_df['url']:
  result = extract_korean_news(url)
  if result:
    news_data.append(result)

df = pd.DataFrame(news_data)

print(f"ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ ê³ ìœ  URL ê°œìˆ˜: {news_url_df['url'].nunique()} / ì „ì²´ í–‰ ê°œìˆ˜: {len(news_url_df)}")
print(f"ë‘ ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ ê³ ìœ  URL ê°œìˆ˜: {df['url'].nunique()} / ì „ì²´ í–‰ ê°œìˆ˜: {len(df)}")

from IPython.display import display_html

news_url_df = news_url_df.drop_duplicates(subset=["url"])
df = df.drop_duplicates(subset=["url"])
herald_df = news_url_df.merge(df, on="url", how="inner")

herald_df

import nest_asyncio
import asyncio
import pandas as pd
import time
from tqdm import tqdm
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# âœ… Colab í™˜ê²½ì—ì„œ asyncio ì¶©ëŒ ë°©ì§€
nest_asyncio.apply()

# âœ… OpenAI API ì„¤ì •
llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
output_parser = StrOutputParser()

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ìµœì í™”)
prompt_template = """Please extract 1 key words from the following content in Korean Hangul (í•œê¸€):
"{text}"
"""
prompt = PromptTemplate.from_template(prompt_template)

# âœ… Chain ì„¤ì •
llm_kwrd = prompt | llm | output_parser

# âœ… ë¹„ë™ê¸° í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (ë”œë ˆì´ ì¶”ê°€)
async def extract_keywords_async(text, index):
    if pd.isna(text) or text.strip() == "":
        print(f"âš ï¸ [{index}] ë¹ˆ í…ìŠ¤íŠ¸ (ê±´ë„ˆëœ€)")
        return ""

    try:
        response = await asyncio.to_thread(llm_kwrd.invoke, {"text": text})
        time.sleep(0.5)  # API ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        print(f"âœ… [{index}] í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {response.strip()}")
        return response.strip()

    except Exception as e:
        print(f"âŒ [{index}] í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

# âœ… tqdm ì„¤ì • (Pandasì™€ í•¨ê»˜ ì‚¬ìš©)
tqdm.pandas()

# âœ… ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ (í•œ ë²ˆì— batch_sizeê°œì”© ìš”ì²­)
async def process_all_texts(df, batch_size=10, save_interval=100):
    results = []

    for i in tqdm(range(0, len(df), batch_size)):
        batch = df.iloc[i : i + batch_size]
        tasks = [extract_keywords_async(text, i + idx) for idx, text in enumerate(batch["content"])]
        batch_results = await asyncio.gather(*tasks)

        results.extend(batch_results)
        df.loc[i : i + batch_size - 1, "keywords"] = batch_results

        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        if (i + batch_size) % save_interval == 0:
            df.to_csv("herald_keywords_progress.csv", index=False, encoding="utf-8-sig")
            print(f"ğŸ’¾ [{i + batch_size}] ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ (herald_keywords_progress.csv)")

        time.sleep(0.5)  # ê° ë°°ì¹˜ ì‹¤í–‰ í›„ 1ì´ˆ ë”œë ˆì´

    return results

# âœ… ì‹¤í–‰ í•¨ìˆ˜ (Colab ì´ë²¤íŠ¸ ë£¨í”„ ë¬¸ì œ í•´ê²°)
def run_keyword_extraction(df):
    loop = asyncio.get_event_loop()  # ê¸°ì¡´ ë£¨í”„ ê°€ì ¸ì˜¤ê¸°
    df["keywords"] = loop.run_until_complete(process_all_texts(df, batch_size=10, save_interval=100))

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    df.to_csv("herald_keywords_final.csv", index=False, encoding="utf-8-sig")
    print(f"âœ… ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ (herald_keywords_final.csv)")
    return df

# âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰
herald_df = run_keyword_extraction(herald_df)

# âœ… ê²°ê³¼ í™•ì¸
print(herald_df)

# âœ… csv íŒŒì¼ë¡œ ì €ì¥
herald_df.to_csv("herald_news.csv", index=False, encoding="utf-8-sig")

# âœ… JSON íŒŒì¼ë¡œ ì €ì¥
herald_df.to_json("herald_news.json", orient="records", force_ascii=False, indent=4)





