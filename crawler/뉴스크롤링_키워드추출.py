# -*- coding: utf-8 -*-
"""뉴스크롤링_키워드추출.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hAFSFOpSjnO38HAocK0J_U99at91VmU0
"""

! pip install webdriver-manager
! apt-get update
! apt-get install -y wget unzip
! pip install requests beautifulsoup4 pandas lxml
! pip install ace_tools

! pip install langchain_openai

import os

os.environ['OPENAI_API_KEY'] = "OPENAI_API_KEY"

import requests
from bs4 import BeautifulSoup
import datetime

# ✅ 언론사 코드 설정 (예: 헤럴드경제 -> 016)
MEDIA_CODE = "016"

# ✅ 크롤링할 기간 설정 (최근 3개월)
today = datetime.date.today()
date_range = [(today - datetime.timedelta(days=i)).strftime('%Y%m%d') for i in range(180)]

# ✅ 기사 URL 리스트 가져오기
def get_news_urls(news_date):
    NAVER_RANKING_URL = f"https://media.naver.com/press/{MEDIA_CODE}/ranking?type=section&date={news_date}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36"
    }

    response = requests.get(NAVER_RANKING_URL, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    news_data = []
    section_map = {
        3: "정치",
        4: "경제",
        5: "사회",
        6: "국제",
        7: "문화",
        8: "과학"
    }

    found_news = False

    for section in range(3, 9):
      for rank in range(1, 4):
        selector = f"#ct > div.press_ranking_home > div:nth-child({section}) > ul > li:nth-child({rank}) > a"
        link = soup.select_one(selector)

        if link:
          url = link.get("href")
          if url and "article" in url:
            full_url = "https://n.news.naver.com" + url if url.startswith("/article") else url

            news_data.append({
                "press": "헤럴드경제",
                "date": news_date,
                "section": section_map.get(section, "기타"),
                "url": full_url
              })
            found_news = True

    if not found_news:
      print(f"{news_date}에 뉴스가 없습니다. 다음 날짜로 이동합니다.")

    return news_data

# ✅ 최근 30일간 뉴스 수집 실행
all_news_urls = []
for news_date in date_range:
    news_urls = get_news_urls(news_date)
    all_news_urls.extend(news_urls)

# ✅ DataFrame 변환 및 출력
news_url_df = pd.DataFrame(all_news_urls)

# ✅ 데이터 확인
from IPython.display import display
display(news_url_df)

import re
import requests
import pandas as pd
from bs4 import BeautifulSoup

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                        'Chrome/102.0.0.0 Safari/537.36'}


def extract_korean_news(url):
    try:
        get_url = requests.get(url=url, headers=headers)
    except requests.exceptions.RequestException as e:
        print("요청 실패")
        return None

    bs = BeautifulSoup(get_url.text, 'lxml')
    content = bs.find('article', id='dic_area')

    if content is None:
      print("본문을 찾을 수 없음")
      return None

    unexpectes = [content.find('strong')] + content.find_all('span', {'class': 'end_photo_org'})
    for exception in unexpectes:
        if exception is not None:
            exception.extract()

    content = content.text
    for i in range(2):
        content = re.sub('  ', ' ', content)
    content = re.sub('\n|\t|\xa0', ' ', content)

    return {'content': content, 'url': url}

news_data = []
for url in news_url_df['url']:
  result = extract_korean_news(url)
  if result:
    news_data.append(result)

df = pd.DataFrame(news_data)

print(f"첫 번째 데이터프레임 고유 URL 개수: {news_url_df['url'].nunique()} / 전체 행 개수: {len(news_url_df)}")
print(f"두 번째 데이터프레임 고유 URL 개수: {df['url'].nunique()} / 전체 행 개수: {len(df)}")

from IPython.display import display_html

news_url_df = news_url_df.drop_duplicates(subset=["url"])
df = df.drop_duplicates(subset=["url"])
herald_df = news_url_df.merge(df, on="url", how="inner")

herald_df

import nest_asyncio
import asyncio
import pandas as pd
import time
from tqdm import tqdm
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# ✅ Colab 환경에서 asyncio 충돌 방지
nest_asyncio.apply()

# ✅ OpenAI API 설정
llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
output_parser = StrOutputParser()

# ✅ 프롬프트 템플릿 설정 (최적화)
prompt_template = """Please extract 1 key words from the following content in Korean Hangul (한글):
"{text}"
"""
prompt = PromptTemplate.from_template(prompt_template)

# ✅ Chain 설정
llm_kwrd = prompt | llm | output_parser

# ✅ 비동기 키워드 추출 함수 (딜레이 추가)
async def extract_keywords_async(text, index):
    if pd.isna(text) or text.strip() == "":
        print(f"⚠️ [{index}] 빈 텍스트 (건너뜀)")
        return ""

    try:
        response = await asyncio.to_thread(llm_kwrd.invoke, {"text": text})
        time.sleep(0.5)  # API 요청 간격 조절
        print(f"✅ [{index}] 키워드 추출 완료: {response.strip()}")
        return response.strip()

    except Exception as e:
        print(f"❌ [{index}] 키워드 추출 실패: {e}")
        return ""

# ✅ tqdm 설정 (Pandas와 함께 사용)
tqdm.pandas()

# ✅ 비동기 배치 처리 함수 (한 번에 batch_size개씩 요청)
async def process_all_texts(df, batch_size=10, save_interval=100):
    results = []

    for i in tqdm(range(0, len(df), batch_size)):
        batch = df.iloc[i : i + batch_size]
        tasks = [extract_keywords_async(text, i + idx) for idx, text in enumerate(batch["content"])]
        batch_results = await asyncio.gather(*tasks)

        results.extend(batch_results)
        df.loc[i : i + batch_size - 1, "keywords"] = batch_results

        # 중간 결과 저장
        if (i + batch_size) % save_interval == 0:
            df.to_csv("herald_keywords_progress.csv", index=False, encoding="utf-8-sig")
            print(f"💾 [{i + batch_size}] 중간 결과 저장 완료 (herald_keywords_progress.csv)")

        time.sleep(0.5)  # 각 배치 실행 후 1초 딜레이

    return results

# ✅ 실행 함수 (Colab 이벤트 루프 문제 해결)
def run_keyword_extraction(df):
    loop = asyncio.get_event_loop()  # 기존 루프 가져오기
    df["keywords"] = loop.run_until_complete(process_all_texts(df, batch_size=10, save_interval=100))

    # 최종 결과 저장
    df.to_csv("herald_keywords_final.csv", index=False, encoding="utf-8-sig")
    print(f"✅ 최종 결과 저장 완료 (herald_keywords_final.csv)")
    return df

# ✅ 키워드 추출 실행
herald_df = run_keyword_extraction(herald_df)

# ✅ 결과 확인
print(herald_df)

# ✅ csv 파일로 저장
herald_df.to_csv("herald_news.csv", index=False, encoding="utf-8-sig")

# ✅ JSON 파일로 저장
herald_df.to_json("herald_news.json", orient="records", force_ascii=False, indent=4)





